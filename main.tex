\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{indentfirst}
%\usepackage[margin=1in]{geometry}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{BIOSTAT M235 - Final Project}
\author{Peike Li, Darren Lin, Timothy Shen}
\date{June 2025}

\begin{document}
\doublespacing


\maketitle

\newpage

\section{Introduction} \label{sec:intro}

In the paper “A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data” by Elaine L. Zanutto, the author compares the use of propensity score analysis and multiple linear regression (MLR), with and without weights \cite{zanutto_2022}. Both of these analysis methods require the use of survey weights when using complex survey data. Survey weights represent the number of individuals in a population that a single subpopulation in a survey represents. Since the covariates used in the naive modeling do not account for survey weights, there may be model misspecification, and the treatment effects may be biased.

In the paper, Zannuto aims to compare these different modeling methods by applying them to the 1997 SESTAT database. The primary goal of her analysis is to understand the wage difference in men and women in different occupations in the field of computer science and to understand the advantages and disadvantages of each method.


\section{Set Up} \label{sec:setup}

In a MLR model, the treatment effect can be estimated from the coefficient of the treatment assignment variable (often an indicator function). The model is usually in the form $Y_i = \beta_0 + \beta_1 I(treatment = 1) + \beta_2 X_2 + ... + \beta_k X_k$ where $\beta_1$ is the treatment effect.

In a propensity score model, the treatment propensity score model is in the form of $e(X) = \mathbb{P}(T = 1 \mid X)$, which is calculated from the logistic regression model $\log\left( \frac{e(X)}{1 - e(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p$. Each unit is then divided into different strata based on their propensity scores. The average treatment effect (ATE) is calculated as $ \Delta_1 = \sum_{k=1}^K \frac{n_{0k}}{N_0}(\bar{y}_{1k} - \bar{y}_{0k})$ where $k$ is the stratum, $\bar{y}_{1k}$ and $\bar{y}_{0k}$ are the mean outcome values of the treatment and control groups respectively, $n_{1k}$ is the number of treated units, and $N_1$ is the total number of treated units. The standard error is given by $\hat{s}(\Delta_1) = \sqrt{\sum_{k=1}^K \frac{n_{0k}^2}{N_0^2} (\frac{s^2_{1k}}{n_{1k}} + \frac{s^2_{0k}}{n_{0k}})}$. If there are still differences in covariate balance after propensity score modeling, the model can be reestimated using regression adjustments so that the ATE is $\Delta_2 = \sum_{k=1}^s \frac{n_{1k}}{N_1} \hat{\beta}_{k,male}$ 

When using survey weights, the ATE becomes

\begin{equation}
\Delta_{w1} = \sum_{k=1}^{K} \left( 
\frac{\sum_{i \in S0_k} w_i}{\sum_{k=1}^{K} \sum_{i \in S0_k} w_i} 
\right) \left( 
\frac{\sum_{i \in S1_k} w_i y_i}{\sum_{i \in S1_k} w_i} - 
\frac{\sum_{i \in S0_k} w_i y_i}{\sum_{i \in S0_k} w_i} 
\right) 
\label{eq:ate_weights}
\end{equation}
where $w_i$ are the survey weights for unit $i$ and $S_{1k}$ and $S_{0k}$ are the sets of treatment or control units respectively that are in stratum $k$. If a regression adjustment is used, the ATE instead becomes 

\begin{equation}
\Delta_{w2} = \sum_{k=1}^{5} 
\left( 
\frac{\sum_{i \in S_{0k}} w_i}{\sum_{k=1}^{5} \sum_{i \in S_{0k}} w_i} 
\right) 
\hat{\beta}^{w}_{k,1}
\label{eq:ate_regadj}
\end{equation}



\section{Methodology and Main Analysis} \label{sec:meth_analysis}

The paper evaluated these methods by estimating gender salary gaps across four IT occupations in the 1997 U.S. SESTAT survey, incorporating its survey weights. As a first step, the unadjusted gender gap was estimated by calculating difference of the weighted average salaries for men and women—showing women earned $7\%\text{--}12\%$ less than men across occupations. Then, to better isolate the gender effect, the authors accounted for potential confounding variables--including education and job-related covariates by running both MLR and propensity score methods.

First, survey-weighted MLRs were fitted for each occupation, with gender and its interactions. Wald F-tests assessed interaction significance. Then backward selection was applied to identify significant predictors. Diagnostic plots verified model specifications. The regression results showed significant gender salary gaps in all four occupations. 

Next, propensity score analysis was used. For each occupation, they estimated the propensity score using an unweighted logistic regression that included the main effects for all covariates. The estimated propensity scores were used to subclassify the sample into five strata based on quintiles.

Covariate balance was assessed using two-way ANOVAs for continuous variables and survey-weighted logistic regressions for binary ones, with the covariate as the outcome and gender, propensity score stratum, and their interaction as predictors. Nonsignificant gender and interaction terms indicated balance; otherwise, interaction and quadratic terms were added. Remaining imbalances were addressed by fitting survey-weighted MLRs within each stratum, regressing salary on gender and unbalanced covariates. The resulting gender coefficients were then used in equation \eqref{eq:ate_regadj} to estimate the overall gender salary gap. The propensity score analysis also revealed significant gender salary gaps across all four occupations.

Lastly, comparisons of weighted vs. unweighted models revealed that ignoring survey design led to differing results. Unweighted estimates of the gender salary gap were larger for computer programmers and software engineers, but smaller for computer systems analysts and information systems scientists.

\section{Discussion} \label{sec:discussion}

In observational studies, MLR is a common method for estimating treatment effects \cite{doi:https://doi.org/10.1002/0470090456.ch5}. However, propensity score methods, with their matched design, can reduce bias and confounding \cite{rosenbaum19834}. In fact, in the field of public policy and epidemiology, the propensity score methodology has become more common. Thus, the paper offers a meaningful comparison of methods to complex survey design data. Propensity score modeling with survey weights can provide meaningful advantages over the conventional MLR. However, certain limitations remain.

\subsection{Advantages} \label{subsec:advantages}

%\subsubsection{Population Level Estimate} \label{subsubsec:pop_lev_est}

\textbf{Population Level Estimate}: Observational data from a complex survey design implies randomness is not achieved--making it difficult extrapolate for a population level estimate. Incorporating survey weights counteracts this. In fact, as discussed in Section \ref{sec:meth_analysis}, the weighted and unweighted analyses led to differing results. The difference was caused by the underrepresentation of the lower-paid men and women relative to the population, implying that the unweighted results did a poorer job reflecting the population.

%\subsubsection{Unbiasedness} \label{subsubsec:unbias}

\textbf{Unbiasedness}: Given that propensity score analysis stratifies units by their estimated propensity scores, it serves as a balance score implying that the observed covariates is independent of the assignment mechanism. Thus, within strata, since it is homogeneous in the propensity score, the distributions of the covariates are the same between the treated and the control. When the strong ignorability assumption holds, differences in outcomes between treated and control units within strata provide unbiased estimates of the ATE.

%\subsubsection{Robustness and Modeling Assumption} \label{subsubsec:robust}

\textbf{Robustness in Modeling Assumption}: While propensity scores depend on model specification, the main goal is covariate balance, not modeling the outcome. Thus unlike linear regression, which assumes a functional form between covariates and outcome, propensity score methods are more robust to misspecification. Furthermore, if balance is not achieved, regression adjustments within strata can be used. However, since units in each stratum are more similar, these regression models are less sensitive to misspecification.


%\subsubsection{Separation of the Modeling and Outcome Analysis} \label{subsubsec:sep}

\textbf{Sepration of Model and Outcome Analysis}: Propensity score models are constructed to achieve balance, and thus does not involve the outcome data. Therefore, the analysis can be more objective since there is a complete separation between the modeling and the outcome.

\subsection{Limitations} \label{subsec:limitations}

Propensity score methods primarily estimate ATE and may miss important interactions. For example, the gender salary gap may vary by region or experience, which a single average cannot capture. In contrast, linear regression can directly model such interactions. Additionally, omitted confounders or post-treatment variables can bias results. For instance, including job rank—potentially influenced by gender bias—may understate true differences. These concerns, however, apply to both MLR and propensity score methods.

\section{Comments} \label{sec:finalcomments}

The paper finds agreement between MLR and propensity score methods on the outcome of gender salary gaps. However, both methods rely on the assumption that all relevant confounders are observed and controlled. As discussed in Section \ref{subsec:limitations}, important covariates can be omitted. In the context of the paper, the exclusion of covariates like workplace culture or discrimination, can yield biased estimates for both. In practice, it would likely be beneficial to complement these methods with sensitivity analysis to understand the effects of these variables. Furthermore, future work could explore extending methods such as doubly robust estimators for complex survey design. It is of note, however, the likely reason that both methods agree is due to good overlap in covariate balance. Thus, furthermore, a good addition to the paper would be understanding the difference in methodology under the context of poor covariate balances. 

Overall, in many fields, randomized controlled trials are often infeasible for estimating treatment effects. Researchers therefore rely on observational data, which frequently come from complex surveys. However, such data frequently come from complex survey designs, where the use of survey weights is essential for producing valid population-level inferences. In such cases, survey weights are essential for valid population-level inferences—particularly in public health, where large-scale surveys are common. This paper presents a proof-of-concept for applying propensity score methods with survey weights in these contexts.

\section{Extension}

\subsection{Motivation}

The Youth Risk Behavior Surveillance System (YRBSS), developed by the Centers for Disease Control and Prevention (CDC), is a national complex survey design that monitors health-related behaviors among U.S. high school students. These behaviors include substance use, dietary habits, physical activity, and mental health indicators. The dataset has spurred interest in identifying environmental and social risk factors in adolescence mental healths. 

In fact, in the most recent report of CDC's Morbidity and Mortality Weekly Report found that students who reported being bullied at school had significantly higher rates of persistent sadness or hopelessness compared to those who were not bullied, even after adjusting for demographics and other behaviors. However, to our knowledge, all analysis were derived from logistic regression models and Chi-square tests. Therefore, we extend propensity score methods to this context by incorporating the survey design and weights--providing a causal estimate between bullying and adolescent mental health.

Furthermore, our model will use a binary outcome instead of a continuous outcome used in Zanutto's paper.

\subsection{Method}

The 2023 YRBSS dataset used here contains individual-level responses with 20,103 observations and 250 columns, which includes mostly binary and ordinal variables. Sampling weights are provided to account for survey design, nonresponse, and post-stratification adjustments [CITE].

Our primary outcome variable is persistent feelings of sadness or hopelessness ("During the past 12 months, did you ever feel so sad or hopeless almost every day for two weeks or more in a row that you stopped doing some usual activities?"). Our treatment variable is the binary indicator of being bullied at school ("During the past 12 months, have you ever been bullied on school property?"). Because of this, our effect of interest is the risk difference of hopelessness between those who were bullied and those who were not. Note for our analysis electronic bullying is not included in this treatment.

Aligning with existing variable selection, we include separate covariates to control for confounding effects. Those variables will be separated into three broad areas: demographic variables, social environment variables, and health behaviors (see Github for detailed variable breakdown).

Prior to analysis, a data cleaning process was conducted to prepare the dataset. In addition, some multiple-response categorical variables were recoded into binary indicators. For example, the race variable—originally a multiple-choice response—was transformed into a binary variable indicating whether a student identified as white versus non-white.

Similar to Zanutto's paper, we run four models: unweighted and survey-weighted regression and propensity score models. Additionally, we calculated the weighted mean difference to use as a reference. However, given our outcome variable is binary, we run will run a logistic regression model instead of MLR. The model is then of the form $\log(\frac{p_i}{1-p_i}) = \beta_0 + \beta_1I(treatment = 1) + \beta_2X + ... + \beta_kX_k$ where $\beta_1$ is the treatment effect. 

For our regression models, we regressed our outcome (feeling sad or hopeless) on our treatment - bullying - and various covariates of interest that we determined to be significantly associated with feelings of hopelessness. We fit both a weighted and unweighted regression model. Because the coefficient for our treatment does not directly translate to a difference in our outcome risk, we calculated the difference in marginal risks under the treatment and control to find the ATE.

The propensity score model will follow the same set up as described in Section \ref{sec:setup}. However, now the ATE (Eq. \ref{eq:ate_weights}) represents as population-level risk differences.

\subsection{Results}

Our results show that there is a difference in the estimates between each method.

\subsection{Discussion}

\input{summary_results_table.tex}

\bibliographystyle{plain}  % or "apalike", "ieeetr", "acm", etc.
\bibliography{references}


\end{document}
